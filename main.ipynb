{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET MACH_BDA_DATAVIZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"d0r1h/customer_churn\")\n",
    "\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "df_reset = df.copy() # copie pour ne pas recharger tout le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_dataframe():\n",
    "    return df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(columns=['security_no', 'referral_id'],inplace=True)\n",
    "    \n",
    "    df['medium_of_operation'] = df['medium_of_operation'].replace('?', 'Unknown')\n",
    "    \n",
    "    df = df[df['days_since_last_login'] > 0]\n",
    "    df = df[df['avg_time_spent'] > 0]\n",
    "    df = df[df['points_in_wallet'] > 0]\n",
    "    df = df[df['avg_frequency_login_days'] != \"Error\"]\n",
    "    \n",
    "    df['avg_frequency_login_days'] = pd.to_numeric(df['avg_frequency_login_days'])\n",
    "    \n",
    "    df['joining_date'] = pd.to_datetime(df['joining_date'], format='%d-%m-%Y')\n",
    "    df['last_visit_time'] = pd.to_datetime(df['last_visit_time'], format='%H:%M:%S').dt.time\n",
    "\n",
    "    df['joining_year'] = df['joining_date'].dt.year\n",
    "    df['joining_month'] = df['joining_date'].dt.month\n",
    "    df['joining_day'] = df['joining_date'].dt.day\n",
    "\n",
    "    df['last_visit_hour'] = df['last_visit_time'].apply(lambda x: x.hour)\n",
    "    df['last_visit_minute'] = df['last_visit_time'].apply(lambda x: x.minute)\n",
    "\n",
    "    df.drop(columns=['joining_date', 'last_visit_time'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisation des données numériques (mettre toutes les valeurs entre 0 et 1, pas sûr s'il y a vraiment besoin):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numeric_columns(df):\n",
    "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_columns] = MinMaxScaler().fit_transform(df[numeric_columns])\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage des données non numériques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_columns(df):\n",
    "    non_numeric_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    dict_encode = { value: index for index, value in enumerate(non_numeric_columns) }\n",
    "    \n",
    "    encoded_df = df.copy()\n",
    "    \n",
    "    for col in dict_encode:\n",
    "        l_occurences = df[col].unique()\n",
    "        dict_encode[col] = { value: index for index, value in enumerate(l_occurences) }\n",
    "        encoded_df[col] = encoded_df[col].apply(lambda x : dict_encode[col][x])\n",
    "    \n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reset_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse et visualisation exploratoire des Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrice de corrélation pour voir si certaines features sont redondantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encode_columns(df)\n",
    "\n",
    "corr_matrix = df.corr().reset_index().melt('index')\n",
    "corr_matrix.columns = ['Variable1', 'Variable2', 'Correlation']\n",
    "corr_matrix['Correlation'] = corr_matrix['Correlation'].round(2)\n",
    "\n",
    "heatmap = alt.Chart(corr_matrix).mark_rect().encode(\n",
    "    x='Variable1:O',\n",
    "    y='Variable2:O',\n",
    "    color=alt.Color('Correlation:Q', scale=alt.Scale(scheme='turbo')),\n",
    "    tooltip=['Variable1', 'Variable2', 'Correlation']\n",
    ").properties(\n",
    "    width=1000,\n",
    "    height=1000\n",
    ")\n",
    "\n",
    "text = heatmap.mark_text(baseline='middle').encode(\n",
    "    text='Correlation:Q',\n",
    "    color=alt.condition(\n",
    "        alt.datum.Correlation > 0.5,\n",
    "        alt.value('white'),\n",
    "        alt.value('black'),\n",
    "        scale=alt.Scale(domain=[0, 1], range=['black', 'white'])\n",
    "    )\n",
    ")\n",
    "heatmap + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Désactiver la limite de lignes\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "df = reset_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age des clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_age = alt.Chart(df).mark_rect().encode(\n",
    "    alt.X('age:Q', bin=alt.Bin(maxbins=30), title='Âge'),\n",
    "    alt.Y('count()', title='Fréquence'),\n",
    "    color=alt.Color('age', scale=alt.Scale(scheme='viridis'))\n",
    ").properties(\n",
    "    title='Distribution de l\\'âge des clients',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "hist_age.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution des genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_gender = alt.Chart(df).mark_arc(outerRadius=120).encode(\n",
    "    theta='count()',\n",
    "    color='gender:N',\n",
    "    tooltip=['gender', 'count()']\n",
    ").properties(\n",
    "    title='Répartition des genres'\n",
    ")\n",
    "\n",
    "pie_gender\n",
    "\n",
    "# la partie unknown est très petite, elle est en haut avec 35 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catégorie de région"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_region = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('count()', title='Fréquence'),\n",
    "    y=alt.Y('region_category:N', title='Catégorie de région'),\n",
    "    color='region_category:N'\n",
    ").properties(\n",
    "    title='Distribution par catégorie de région',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "bar_region.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catégorie d'abonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_membership = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('count()', title='Fréquence'),\n",
    "    y=alt.Y('membership_category:N', title='Catégorie d\\'abonnement'),\n",
    "    color=alt.Color('age', scale=alt.Scale(scheme='turbo'))\n",
    ").properties(\n",
    "    title='Distribution par catégorie d\\'abonnement',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "bar_membership.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_counts = df['feedback'].value_counts().reset_index()\n",
    "feedback_counts.columns = ['feedback', 'count']\n",
    "\n",
    "color_scale = alt.Scale(\n",
    "    domain=[\n",
    "        'No reason specified', 'Poor Customer Service', 'Poor Product Quality',\n",
    "        'Poor Website', 'Too many ads', 'Products always in Stock', 'Quality Customer Care',\n",
    "        'Reasonable Price', 'User Friendly Website'\n",
    "    ],\n",
    "    range=[\n",
    "        '#00008b', '#ff0000', '#ff0000', '#ff0000', '#ff0000', '#00ff00', '#00ff00', '#00ff00', '#00ff00'\n",
    "    ]\n",
    ")\n",
    "\n",
    "bar_feedback = alt.Chart(feedback_counts).mark_bar().encode(\n",
    "    x='feedback:N',\n",
    "    y='count:Q',\n",
    "    tooltip=['feedback', 'count'],\n",
    "    color=alt.Color('feedback:N', scale=color_scale)\n",
    ").properties(\n",
    "    title='Répartition des feedbacks',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "bar_feedback.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points dans le portefeuille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_points_wallet = alt.Chart(df).mark_rect().encode(\n",
    "    alt.X('points_in_wallet:Q', bin=alt.Bin(maxbins=30), title='Points dans le portefeuille'),\n",
    "    alt.Y('count()', title='Nombre de clients'),\n",
    "    color=alt.Color('points_in_wallet', scale=alt.Scale(scheme='blues'))\n",
    ").properties(\n",
    "    title='Distribution des points dans le portefeuille',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "hist_points_wallet.display()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation de plusieurs modèles avec les bonnes métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encode_columns(df)\n",
    "df = normalize_numeric_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['churn_risk_score'])\n",
    "y = df['churn_risk_score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVM': SVC(kernel='linear'),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    results[model_name] = accuracy\n",
    "\n",
    "for model_name, accuracy in results.items():\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation des meilleurs modèles (GDBoosting et RandomForest) pour améliorer leur accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GradientBoostingClassifier()\n",
    "\n",
    "# param_dist = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'max_depth': [3, 4, 5],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'subsample': [0.8, 0.9, 1.0]\n",
    "# }\n",
    "\n",
    "# random_search_gdb = RandomizedSearchCV(estimator=model, param_distributions=param_dist, \n",
    "#                                    n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42, scoring='accuracy')\n",
    "\n",
    "# random_search_gdb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best parameters found: \", random_search_gdb.best_params_)\n",
    "\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(random_search_gdb.best_score_))\n",
    "\n",
    "# best_model_gdb = random_search_gdb.best_estimator_\n",
    "# y_pred_gdb = best_model_gdb.predict(X_test)\n",
    "# accuracy_gdb = accuracy_score(y_test, y_pred_gdb)\n",
    "\n",
    "# print(\"Test set accuracy: {:.2f}\".format(accuracy_gdb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found: {'subsample': 1.0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 5, 'learning_rate': 0.1}\n",
    "Best cross-validation score: 0.94\n",
    "Test set accuracy: 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier()\n",
    "\n",
    "# param_distributions = {\n",
    "#     'n_estimators': np.arange(100, 501, 50),\n",
    "#     'max_depth': np.arange(3, 21, 2),\n",
    "#     'min_samples_split': np.arange(2, 21, 2),\n",
    "#     'min_samples_leaf': np.arange(1, 11, 1),\n",
    "#     'bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "# random_search_rf = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=100, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "# random_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best parameters found: \", random_search_rf.best_params_)\n",
    "\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(random_search_rf.best_score_))\n",
    "\n",
    "# best_model_rf = random_search_rf.best_estimator_\n",
    "# y_pred_rf = best_model_rf.predict(X_test)\n",
    "# accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# print(\"Test set accuracy: {:.2f}\".format(accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found:  {'n_estimators': 250, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_depth': 19, 'bootstrap': False}\n",
    "Best cross-validation score: 0.94\n",
    "Test set accuracy: 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_importance(model,coeff_method=None):\n",
    "    if coeff_method == 'arbres':\n",
    "        feature_importances = model.feature_importances_\n",
    "    else:\n",
    "        feature_importances = model.coef_[0]\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = features_importance(models['Random Forest'], 'arbres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser et interpreter les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_chart = alt.Chart(feature_importance_df).mark_bar().encode(\n",
    "    x=alt.X('Importance:Q', title='Importance'),\n",
    "    y=alt.Y('Feature:O', title='Feature', sort='-x'),\n",
    "    tooltip=['Feature', 'Importance']\n",
    ").properties(\n",
    "    title='Feature Importances',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "feature_importance_chart.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
